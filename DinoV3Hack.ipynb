{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No5B3lKAP8Gn"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install transformers rasterio scikit-image opencv-python-headless faiss-gpu-cu12 streamlit\n",
        "!pip install rasterio scikit-image opencv-python-headless faiss-gpu-cu12 streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPPQQH1Yk55"
      },
      "source": [
        "## Mounting drive and downloading checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpWu3EZgYquQ",
        "outputId": "30f24147-070d-4723-ac45-f42f74143540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nwtqUpUSYqr2"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# SAVE_DIR = '/content/drive/MyDrive/grand_ai_challenge/dinov3_ckpts'\n",
        "# os.makedirs(SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu-s1pgDYqpT",
        "outputId": "8d5ad9af-f117-41f6-b893-c25ccf8790f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-09-09 16:55:06--  https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiamhlYmE4Njd6d2t5ZXp2ZWx5dmx6ODRlIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTc1ODc4MTh9fX1dfQ__&Signature=IAHroiGoyVcSvf3V7g5FEQroEHM6pWocZjc07TGmcQDjAHcdLR0lUMwi-zKJgE6aZYkKHvpHcJTA2iD5qovlIHacog5m6ep7Nl0tino91DRXQh82z%7EWjmaaMwRZ36QPbBUUI-GvOzG0Uxya9Spzcw9kRc4APl-6vkvdOdnKaukgkouzqmiqRtW1oEBE9t%7ERjIevcgvlHrnzfpDTJHs4AY8bvCXa67s6IELFMg5XxIWTpb7sLi3vKPbQJ2bzMcXOENOgrOl1jN-5huRfL%7EhS3ZBVLM9wUPecgOSxRmhRAnt9s9cMdbxQs15I01xwxXkPlabxrx8BgJmvQ7bDLSf6cAg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1468529534383439\n",
            "Resolving dinov3.llamameta.net (dinov3.llamameta.net)... 13.249.126.80, 13.249.126.33, 13.249.126.25, ...\n",
            "Connecting to dinov3.llamameta.net (dinov3.llamameta.net)|13.249.126.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1213059235 (1.1G) [binary/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/grand_ai_challenge/dinov3_ckpts/dinov3-vitl16-pretrain-sat493m.pth’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   1.13G  72.0MB/s    in 16s     \n",
            "\n",
            "2025-09-09 16:55:23 (72.3 MB/s) - ‘/content/drive/MyDrive/grand_ai_challenge/dinov3_ckpts/dinov3-vitl16-pretrain-sat493m.pth’ saved [1213059235/1213059235]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CKPT_URL = 'https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiamhlYmE4Njd6d2t5ZXp2ZWx5dmx6ODRlIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTc1ODc4MTh9fX1dfQ__&Signature=IAHroiGoyVcSvf3V7g5FEQroEHM6pWocZjc07TGmcQDjAHcdLR0lUMwi-zKJgE6aZYkKHvpHcJTA2iD5qovlIHacog5m6ep7Nl0tino91DRXQh82z%7EWjmaaMwRZ36QPbBUUI-GvOzG0Uxya9Spzcw9kRc4APl-6vkvdOdnKaukgkouzqmiqRtW1oEBE9t%7ERjIevcgvlHrnzfpDTJHs4AY8bvCXa67s6IELFMg5XxIWTpb7sLi3vKPbQJ2bzMcXOENOgrOl1jN-5huRfL%7EhS3ZBVLM9wUPecgOSxRmhRAnt9s9cMdbxQs15I01xwxXkPlabxrx8BgJmvQ7bDLSf6cAg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1468529534383439'\n",
        "# CKPT_PATH = os.path.join(SAVE_DIR, 'dinov3-vitl16-pretrain-sat493m.pth')\n",
        "# !wget -O \"$CKPT_PATH\" \"$CKPT_URL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "drpQRu_HfOvL"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p ~/.cache/torch/hub/checkpoints\n",
        "# !cp \"$CKPT_PATH\" ~/.cache/torch/hub/checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V1kQZdpfSXS"
      },
      "source": [
        "## Clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWDNaIOwfUUq",
        "outputId": "0184659f-0dc4-4c8c-e287-ecfa54a79d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/grand_ai_challenge\n",
            "Cloning into 'dinov3'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 334 (delta 77), reused 32 (delta 32), pack-reused 212 (from 2)\u001b[K\n",
            "Receiving objects: 100% (334/334), 9.82 MiB | 9.33 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n",
            "Updating files: 100% (163/163), done.\n"
          ]
        }
      ],
      "source": [
        "# 5) Clone DINOv3 repo locally\n",
        "%cd /content/drive/MyDrive/grand_ai_challenge\n",
        "!rm -rf dinov3\n",
        "!git clone https://github.com/facebookresearch/dinov3.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zhcDjlm4geJg",
        "outputId": "33032abf-7ae7-480b-c1a8-fe9db0bc1b71"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/grand_ai_challenge/dinov3_ckpts/dinov3-vitl16-pretrain-sat493m.pth'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CKPT_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5RG35DzfUSH",
        "outputId": "6488d271-7146-4e57-ca3b-e200699ddb3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiamhlYmE4Njd6d2t5ZXp2ZWx5dmx6ODRlIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTc1ODc4MTh9fX1dfQ__&Signature=IAHroiGoyVcSvf3V7g5FEQroEHM6pWocZjc07TGmcQDjAHcdLR0lUMwi-zKJgE6aZYkKHvpHcJTA2iD5qovlIHacog5m6ep7Nl0tino91DRXQh82z%7EWjmaaMwRZ36QPbBUUI-GvOzG0Uxya9Spzcw9kRc4APl-6vkvdOdnKaukgkouzqmiqRtW1oEBE9t%7ERjIevcgvlHrnzfpDTJHs4AY8bvCXa67s6IELFMg5XxIWTpb7sLi3vKPbQJ2bzMcXOENOgrOl1jN-5huRfL%7EhS3ZBVLM9wUPecgOSxRmhRAnt9s9cMdbxQs15I01xwxXkPlabxrx8BgJmvQ7bDLSf6cAg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1468529534383439\" to /root/.cache/torch/hub/checkpoints/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.13G/1.13G [00:05<00:00, 231MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DinoVisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (rope_embed): RopePositionEmbedding()\n",
              "  (blocks): ModuleList(\n",
              "    (0-23): 24 x SelfAttentionBlock(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): SelfAttention(\n",
              "        (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): LayerScale()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): LayerScale()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (local_cls_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Identity()\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "REPO_DIR = '/content/drive/MyDrive/grand_ai_challenge/dinov3'\n",
        "import torch\n",
        "\n",
        "# 6) Direct URL load (no local file), replace CKPT_URL with the real link from the access email\n",
        "dinov3_vits16 = torch.hub.load(\n",
        "    REPO_DIR,\n",
        "    'dinov3_vitl16',\n",
        "    source='local',\n",
        "    weights=CKPT_URL,  # direct URL supported by README\n",
        ")\n",
        "dinov3_vits16.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRALk49lqr77"
      },
      "source": [
        "## Huggingface login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH-nS88YMKmv",
        "outputId": "7cfe5b38-8ef0-4c24-da09-117243eb0f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "!huggingface-cli login --token=$HF_TOKEN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "0ffa56e8b80b402fa5d59882ff5946b5",
            "1b05ae6fe3ea453499dfefcf7e3e6439",
            "3a4c7c6ef6554b28b9bcc16b16201deb",
            "8dafada2c0fe446d9a2150f63ddde520",
            "a0350b45f8474dd19bd8652f8a80f23b",
            "ea4c76d3af0942b482c85497ccefd7ee",
            "d8117b1398224536af15b05be6b3d6df",
            "cb0d3306c0584344a00ac2888965ea2b",
            "a5da411081e14e95aac91af6285f9e7b",
            "7b89dfa8372042dd83fd95cfb7c03847",
            "c23c5a0f2e06417cbe63799680a9a3d5",
            "a7ed36929cc04d3593f99c64936b0370",
            "9fe3ca4d9f0b48679c28ca02cc66bc7a",
            "388d13ed0d2648738afd3c4db0a636ba",
            "e109626c674c418782e0a786cff94222",
            "d82e125834234827b78cced723c9bb5d",
            "ff0f8a3eeeba48f1b16d166c2a5e5a9b"
          ]
        },
        "id": "C0Wn-LgAJeRB",
        "outputId": "f51511c4-ad08-45ce-c0b4-09f714f0bfa3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ffa56e8b80b402fa5d59882ff5946b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PtHrt4sJIBsS"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "def make_transform_sat(resize_size=224):\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((resize_size, resize_size), antialias=True),\n",
        "        transforms.Normalize(mean=(0.430, 0.411, 0.296), std=(0.213, 0.156, 0.143)),\n",
        "    ])  # SAT-493M [3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ar-s1SKiQY28"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "def load_dinov3(model_id=\"facebook/dinov3-vitl16-pretrain-sat493m\", dtype=torch.bfloat16, device=\"cuda\"):\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    model = AutoModel.from_pretrained(model_id, torch_dtype=dtype).to(device).eval()\n",
        "    return processor, model\n",
        "\n",
        "def load_dinov3_local(model_id=\"dinov3-vitl16-pretrain-sat493m\", dtype=torch.bfloat16, device=\"cuda\"):\n",
        "    model_folder = f\"/content/drive/MyDrive/grand_ai_challenge/{model_id}\"\n",
        "    processor = AutoImageProcessor.from_pretrained(model_folder)\n",
        "    model = AutoModel.from_pretrained(model_folder, torch_dtype=dtype).to(device).eval()\n",
        "    return processor, model\n",
        "    # Save locally\n",
        "    # model.save_pretrained(\"/content/drive/MyDrive/grand_ai_challenge/dinov3_ckpts\")\n",
        "    # processor.save_pretrained(\"/content/drive/MyDrive/grand_ai_challenge/dinov3_ckpts\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G48i1PlyQaMB"
      },
      "outputs": [],
      "source": [
        "# four_channel_patch_embed.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def adapt_vit_patch_embed_to_4ch(model):\n",
        "    # Works with HF DINOv3 ViT: find the patch projection conv\n",
        "    # typical path: model.vit.embeddings.patch_embeddings.projection\n",
        "    pe = model.vit.embeddings.patch_embeddings\n",
        "    old = pe.projection  # Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=16, stride=16)\n",
        "\n",
        "    new_conv = nn.Conv2d(\n",
        "        in_channels=4,\n",
        "        out_channels=old.out_channels,\n",
        "        kernel_size=old.kernel_size,\n",
        "        stride=old.stride,\n",
        "        padding=old.padding,\n",
        "        bias=(old.bias is not None),\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w = old.weight  # [out_c, 3, k, k]\n",
        "        w_mean = w.mean(dim=1, keepdim=True)  # [out_c, 1, k, k]\n",
        "        w4 = w_mean.repeat(1, 4, 1, 1)       # [out_c, 4, k, k]\n",
        "        new_conv.weight.copy_(w4)\n",
        "        if old.bias is not None:\n",
        "            new_conv.bias.copy_(old.bias)\n",
        "\n",
        "    pe.projection = new_conv\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqMTjLb2QaOh",
        "outputId": "80335fd2-d520-478c-9df9-c72dceebd18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.4.3)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.8.3)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.2.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (1.26.4)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio) (1.1.1.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.3)\n"
          ]
        }
      ],
      "source": [
        "# !pip install rasterio\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def read_tiff_4ch(path, to_dtype=torch.float32):\n",
        "    with rasterio.open(path) as src:\n",
        "        arr = src.read()  # shape: (bands, H, W)\n",
        "    # Expect 4 bands: [B,G,R,NIR] or similar; reorder to [R,G,B,NIR] if needed\n",
        "    # Here we assume (B,G,R,NIR) -> (R,G,B,NIR)\n",
        "    if arr.shape == 4:\n",
        "        b,g,r,nir = arr\n",
        "        arr = np.stack([r,g,b,nir], axis=0)\n",
        "    else:\n",
        "        raise ValueError(\"Expected 4-band imagery\")\n",
        "    arr = arr.astype(np.float32)\n",
        "    # simple normalization to [0,1] if dynamic range known; advanced normalization can be added\n",
        "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)\n",
        "    tensor = torch.from_numpy(arr)  # CHW\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9Au1fb0PQaRS"
      },
      "outputs": [],
      "source": [
        "# retrieval.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def vit_tokens_to_grid(outputs, H, W, patch=16, num_registers=4):\n",
        "    \"\"\"\n",
        "    DINOv3 ViT returns last_hidden_state with [CLS] + 4 registers + N patches.\n",
        "    \"\"\"\n",
        "    tok = outputs.last_hidden_state  # [B, 1+4+N, D]\n",
        "    B, T, D = tok.shape\n",
        "    nH, nW = H // patch, W // patch\n",
        "    patch_tokens = tok[:, 1+num_registers:, :]  # [B, N, D]\n",
        "    grid = patch_tokens.view(B, nH, nW, D).contiguous()  # [B, nH, nW, D]\n",
        "    return grid\n",
        "\n",
        "def compute_query_prototype(chips_emb_list):\n",
        "    # normalize and average for cosine-stable prototype\n",
        "    embs = [F.normalize(e, dim=-1) for e in chips_emb_list]\n",
        "    proto = torch.stack(embs, dim=0).mean(dim=0)\n",
        "    proto = F.normalize(proto, dim=-1)\n",
        "    return proto  # [D]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hxPtkE8qQaT3"
      },
      "outputs": [],
      "source": [
        "def cosine_heatmap(grid, proto):\n",
        "    # grid: [1, nH, nW, D], proto: [D]\n",
        "    g = F.normalize(grid, dim=-1)\n",
        "    p = F.normalize(proto, dim=-1)\n",
        "    heat = torch.einsum('bhwd,d->bhw', g, p)  # inner product == cosine\n",
        "    return heat  # [1, nH, nW]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qE_Mu4xzQaWd"
      },
      "outputs": [],
      "source": [
        "# multiscale.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def resize_chw(img_chw, scale):\n",
        "    C, H, W = img_chw.shape\n",
        "    newH, newW = int(H*scale), int(W*scale)\n",
        "    img = F.interpolate(img_chw.unsqueeze(0), size=(newH, newW), mode='bilinear', align_corners=False)\n",
        "    return img.squeeze(0), (H, W), (newH, newW)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YuchJAhCQaZC"
      },
      "outputs": [],
      "source": [
        "# postprocess.py\n",
        "import numpy as np\n",
        "from skimage.measure import label, regionprops\n",
        "import torch\n",
        "from torchvision.ops import nms\n",
        "\n",
        "def heatmap_to_boxes(heat, thr=0.85, scale=16):\n",
        "    \"\"\"\n",
        "    heat: [nH, nW] similarity map\n",
        "    \"\"\"\n",
        "    mask = (heat >= thr).astype(np.uint8)\n",
        "    lab = label(mask, connectivity=1)\n",
        "    props = regionprops(lab)\n",
        "    boxes = []\n",
        "    scores = []\n",
        "    for p in props:\n",
        "        minr, minc, maxr, maxc = p.bbox\n",
        "        # convert patch coords to pixel coords with patch size (=scale)\n",
        "        x1, y1 = minc*scale, minr*scale\n",
        "        x2, y2 = maxc*scale, maxr*scale\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        # peak score within region\n",
        "        region_scores = heat[minr:maxr, minc:maxc]\n",
        "        scores.append(float(region_scores.max()))\n",
        "    return np.array(boxes, dtype=np.float32), np.array(scores, dtype=np.float32)\n",
        "\n",
        "def nms_boxes(boxes_np, scores_np, iou_thr=0.5):\n",
        "    if len(boxes_np) == 0:\n",
        "        return boxes_np, scores_np\n",
        "    boxes = torch.from_numpy(boxes_np)\n",
        "    scores = torch.from_numpy(scores_np)\n",
        "    keep = nms(boxes, scores, iou_thr)\n",
        "    return boxes[keep].numpy(), scores[keep].numpy()\n",
        "\n",
        "@torch.inference_mode()\n",
        "def embed_chip(model, processor, chip_chw, device):\n",
        "    H, W = chip_chw.shape[1:]\n",
        "    inputs = to_model_inputs(processor, chip_chw, device)\n",
        "    with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "        out = model(**inputs).last_hidden_state\n",
        "    # use CLS token as global or mean of patches; here mean of patch tokens\n",
        "    num_registers = 4\n",
        "    patches = out[:, 1+num_registers:, :]\n",
        "    emb = patches.mean(dim=1).squeeze(0)  # [D]\n",
        "    return emb\n",
        "\n",
        "def to_model_inputs(processor, img_3ch_chw_or_4ch, device):\n",
        "    # processor expects PIL or numpy HWC 3-channel; for 4-channel we bypass image processor normalization,\n",
        "    # use manual normalization and feed as pixel_values directly\n",
        "    if img_3ch_chw_or_4ch.shape == 3:\n",
        "        hwc = img_3ch_chw_or_4ch.permute(1,2,0).cpu().numpy()\n",
        "        inputs = processor(images=hwc, return_tensors=\"pt\")\n",
        "        return {k: v.to(device) for k,v in inputs.items()}\n",
        "    else:\n",
        "        # 4-channel path: assume model patch_embed was adapted; just pass pixel_values\n",
        "        x = img_3ch_chw_or_4ch.unsqueeze(0).to(device)\n",
        "        return {\"pixel_values\": x}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JnW59PG8Qabh"
      },
      "outputs": [],
      "source": [
        "# feature_extractor.py\n",
        "import torch\n",
        "\n",
        "@torch.inference_mode()\n",
        "def extract_dense_grid(model, pixel_inputs, H, W, dtype=torch.bfloat16):\n",
        "    with torch.autocast(\"cuda\", dtype=dtype):\n",
        "        outputs = model(**pixel_inputs)\n",
        "    grid = vit_tokens_to_grid(outputs, H, W, patch=16, num_registers=4)\n",
        "    return grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "djDuMefJ500B"
      },
      "outputs": [],
      "source": [
        "sample_tif1 = '/content/drive/MyDrive/grand_ai_challenge/sample-set/Pond-1-and-2/GC01PS03T0236.tif'\n",
        "sample_tif2 = '/content/drive/MyDrive/grand_ai_challenge/sample-set/Solar-Panel/GC01PS03D0155.tif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kuMuOGjPUFMc",
        "outputId": "8d7bf010-13c6-447c-c1c9-fd80ff688708"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'DinoVisionTransformer' object has no attribute 'vit'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3278323751.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# processor, model = load_dinov3(\"facebook/dinov3-vitl16-pretrain-sat493m\", dtype=torch.bfloat16, device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_transform_sat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdinov3_vits16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt_vit_patch_embed_to_4ch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. Prepare chips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3279917685.py\u001b[0m in \u001b[0;36madapt_vit_patch_embed_to_4ch\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Works with HF DINOv3 ViT: find the patch projection conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# typical path: model.vit.embeddings.patch_embeddings.projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m  \u001b[0;31m# Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=16, stride=16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1962\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1963\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DinoVisionTransformer' object has no attribute 'vit'"
          ]
        }
      ],
      "source": [
        "# Setup DINOv3 with channel adaptation\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor, model = load_dinov3(\"facebook/dinov3-vitl16-pretrain-sat493m\", dtype=torch.bfloat16, device=device)\n",
        "# processor, model = make_transform_sat, dinov3_vits16\n",
        "model = adapt_vit_patch_embed_to_4ch(model)\n",
        "\n",
        "# 1. Prepare chips\n",
        "# chip_paths = [\"./chips/chip1.tiff\", \"./chips/chip2.tiff\"]  # Provide your paths\n",
        "chip_paths = [sample_tif1, sample_tif2]  # Provide your paths\n",
        "\n",
        "chip_embs = []\n",
        "for path in chip_paths:\n",
        "    chip = read_tiff_4ch(path)\n",
        "    if chip.shape == 4 and model.vit.embeddings.patch_embeddings.projection.in_channels == 3:\n",
        "        chip = chip[:3, ...]  # fallback\n",
        "    chip_embs.append(embed_chip(model, processor, chip.to(device), device))\n",
        "query_proto = compute_query_prototype(chip_embs)\n",
        "\n",
        "# 2. Process a target image (multi-scale)\n",
        "target_path = \"./targets/target1.tiff\"\n",
        "img = read_tiff_4ch(target_path)\n",
        "all_boxes, all_scores = [], []\n",
        "for scale in [0.75, 1.0, 1.25]:\n",
        "    im_s, orig_hw, s_hw = resize_chw(img, scale)\n",
        "    Hs, Ws = s_hw\n",
        "    inputs = to_model_inputs(processor, im_s.to(device), device)\n",
        "    with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "        outputs = model(**inputs)\n",
        "    grid = vit_tokens_to_grid(outputs, Hs, Ws, patch=16, num_registers=4)\n",
        "    heat = cosine_heatmap(grid, query_proto).squeeze(0).cpu().numpy()\n",
        "    boxes, scores = heatmap_to_boxes(heat, thr=0.85, scale=16)\n",
        "    # Rescale boxes to original image\n",
        "    if len(boxes):\n",
        "        boxes[:, [0,2]] *= orig_hw[6] / s_hw[6]\n",
        "        boxes[:, [1,3]] *= orig_hw / s_hw\n",
        "        all_boxes.append(boxes)\n",
        "        all_scores.append(scores)\n",
        "if len(all_boxes):\n",
        "    boxes_np = np.concatenate(all_boxes, axis=0)\n",
        "    scores_np = np.concatenate(all_scores, axis=0)\n",
        "    boxes_np, scores_np = nms_boxes(boxes_np, scores_np, iou_thr=0.5)\n",
        "    for b, sc in zip(boxes_np, scores_np):\n",
        "        print(\"Box:\", [int(x) for x in b], \"Score:\", sc)\n",
        "else:\n",
        "    print(\"No detections above threshold\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PsxXd7K_UFIH"
      },
      "outputs": [],
      "source": [
        "# def adapt_vit_patch_embed_to_4ch(model):\n",
        "#     # Works with HF DINOv3 ViT: find the patch projection conv\n",
        "#     # typical path: model.vit.embeddings.patch_embeddings.projection\n",
        "#     pe = model.embeddings.patch_embeddings\n",
        "#     old = pe.projection  # Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=16, stride=16)\n",
        "\n",
        "#     new_conv = nn.Conv2d(\n",
        "#         in_channels=4,\n",
        "#         out_channels=old.out_channels,\n",
        "#         kernel_size=old.kernel_size,\n",
        "#         stride=old.stride,\n",
        "#         padding=old.padding,\n",
        "#         bias=(old.bias is not None),\n",
        "#     )\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         w = old.weight  # [out_c, 3, k, k]\n",
        "#         w_mean = w.mean(dim=1, keepdim=True)  # [out_c, 1, k, k]\n",
        "#         w4 = w_mean.repeat(1, 4, 1, 1)       # [out_c, 4, k, k]\n",
        "#         new_conv.weight.copy_(w4)\n",
        "#         if old.bias is not None:\n",
        "#             new_conv.bias.copy_(old.bias)\n",
        "\n",
        "#     pe.projection = new_conv\n",
        "#     return model\n",
        "import torch.nn as nn\n",
        "def adapt_vit_patch_embed_to_4ch(model: torch.nn.Module) -> torch.nn.Module:\n",
        "    \"\"\"\n",
        "    Modify DINOv3ViTModel's first patch embedding conv to in_channels=4.\n",
        "    Assumes model.embeddings.patch_embeddings is nn.Conv2d(in=3,out=hidden_size,k=s,s=s).\n",
        "    Weight init: average existing 3-channel kernel along channel dim and repeat to 4.\n",
        "    \"\"\"\n",
        "    pe = model.embeddings.patch_embeddings  # Conv2d(3, hidden, k=16, s=16)\n",
        "    assert isinstance(pe, nn.Conv2d), \"Expected Conv2d at model.embeddings.patch_embeddings\"\n",
        "    assert pe.in_channels == 3, f\"Expected in_channels=3, got {pe.in_channels}\"\n",
        "\n",
        "    new_conv = nn.Conv2d(\n",
        "        in_channels=4,\n",
        "        out_channels=pe.out_channels,\n",
        "        kernel_size=pe.kernel_size,\n",
        "        stride=pe.stride,\n",
        "        padding=pe.padding,\n",
        "        dilation=pe.dilation,\n",
        "        groups=pe.groups,\n",
        "        bias=(pe.bias is not None),\n",
        "        padding_mode=pe.padding_mode,\n",
        "        device=pe.weight.device,\n",
        "        dtype=pe.weight.dtype,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w = pe.weight  # [out_c, 3, k, k]\n",
        "        w_mean = w.mean(dim=1, keepdim=True)   # [out_c, 1, k, k]\n",
        "        w4 = w_mean.repeat(1, 4, 1, 1)         # [out_c, 4, k, k]\n",
        "        new_conv.weight.copy_(w4)\n",
        "        if pe.bias is not None:\n",
        "            new_conv.bias.copy_(pe.bias)\n",
        "\n",
        "    # Replace module in-place\n",
        "    model.embeddings.patch_embeddings = new_conv\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hjOZ3OReUFFh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# processor, model = load_dinov3(\"facebook/dinov3-vitl16-pretrain-sat493m\", dtype=torch.bfloat16, device=device)\n",
        "processor, model = load_dinov3_local(\"dinov3-vitl16-pretrain-sat493m\", dtype=torch.bfloat16, device=device)\n",
        "# processor, model = make_transform_sat, dinov3_vits16\n",
        "# model = adapt_vit_patch_embed_to_4ch(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AuMMZFwfUFDj"
      },
      "outputs": [],
      "source": [
        "model = adapt_vit_patch_embed_to_4ch(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8kPIl_aUFBD",
        "outputId": "bfa26494-8a5d-4f24-8d22-6a2d9c2425c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DINOv3ViTModel(\n",
              "  (embeddings): DINOv3ViTEmbeddings(\n",
              "    (patch_embeddings): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (rope_embeddings): DINOv3ViTRopePositionEmbedding()\n",
              "  (layer): ModuleList(\n",
              "    (0-23): 24 x DINOv3ViTLayer(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention): DINOv3ViTAttention(\n",
              "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "      (layer_scale1): DINOv3ViTLayerScale()\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): DINOv3ViTMLP(\n",
              "        (up_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (down_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (act_fn): GELUActivation()\n",
              "      )\n",
              "      (layer_scale2): DINOv3ViTLayerScale()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuh1AwcGUE-f",
        "outputId": "9be926f6-26ae-4056-f2b7-a9bb008ac1fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embeddings.patch_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN8OuipzUE8B",
        "outputId": "292925c3-b600-49f8-e350-a9cbc10b948c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DINOv3ViTModel(\n",
              "  (embeddings): DINOv3ViTEmbeddings(\n",
              "    (patch_embeddings): Conv2d(4, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (rope_embeddings): DINOv3ViTRopePositionEmbedding()\n",
              "  (layer): ModuleList(\n",
              "    (0-23): 24 x DINOv3ViTLayer(\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (attention): DINOv3ViTAttention(\n",
              "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "      (layer_scale1): DINOv3ViTLayerScale()\n",
              "      (drop_path): Identity()\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): DINOv3ViTMLP(\n",
              "        (up_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "        (down_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        (act_fn): GELUActivation()\n",
              "      )\n",
              "      (layer_scale2): DINOv3ViTLayerScale()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aVu08v2mQaeE",
        "outputId": "30ae3819-e389-458b-b004-36a7299532c0"
      },
      "outputs": [],
      "source": [
        "# # cli.py\n",
        "# import argparse, os, torch, numpy as np\n",
        "# # from io_raster import read_tiff_4ch\n",
        "# # from dinov3_backbone import load_dinov3\n",
        "# # from four_channel_patch_embed import adapt_vit_patch_embed_to_4ch\n",
        "# # from retrieval import compute_query_prototype, vit_tokens_to_grid, cosine_heatmap\n",
        "# # from postprocess import heatmap_to_boxes, nms_boxes\n",
        "# # from multiscale import resize_chw\n",
        "# from transformers import AutoImageProcessor\n",
        "\n",
        "# def to_model_inputs(processor, img_3ch_chw_or_4ch, device):\n",
        "#     # processor expects PIL or numpy HWC 3-channel; for 4-channel we bypass image processor normalization,\n",
        "#     # use manual normalization and feed as pixel_values directly\n",
        "#     if img_3ch_chw_or_4ch.shape == 3:\n",
        "#         hwc = img_3ch_chw_or_4ch.permute(1,2,0).cpu().numpy()\n",
        "#         inputs = processor(images=hwc, return_tensors=\"pt\")\n",
        "#         return {k: v.to(device) for k,v in inputs.items()}\n",
        "#     else:\n",
        "#         # 4-channel path: assume model patch_embed was adapted; just pass pixel_values\n",
        "#         x = img_3ch_chw_or_4ch.unsqueeze(0).to(device)\n",
        "#         return {\"pixel_values\": x}\n",
        "\n",
        "# def embed_chip(model, processor, chip_chw, device):\n",
        "#     H, W = chip_chw.shape[1:]\n",
        "#     inputs = to_model_inputs(processor, chip_chw, device)\n",
        "#     with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "#         out = model(**inputs).last_hidden_state\n",
        "#     # use CLS token as global or mean of patches; here mean of patch tokens\n",
        "#     num_registers = 4\n",
        "#     patches = out[:, 1+num_registers:, :]\n",
        "#     emb = patches.mean(dim=1).squeeze(0)  # [D]\n",
        "#     return emb\n",
        "\n",
        "# def main():\n",
        "#     ap = argparse.ArgumentParser()\n",
        "#     ap.add_argument(\"--chips\", nargs=\"+\", required=True)\n",
        "#     ap.add_argument(\"--targets_dir\", required=True)\n",
        "#     ap.add_argument(\"--out\", required=True)\n",
        "#     ap.add_argument(\"--object_name\", required=True)\n",
        "#     ap.add_argument(\"--model_id\", default=\"facebook/dinov3-vitb16-pretrain-lvd1689m\")\n",
        "#     ap.add_argument(\"--satellite_mode\", action=\"store_true\")\n",
        "#     ap.add_argument(\"--thr\", type=float, default=0.85)\n",
        "#     ap.add_argument(\"--nms\", type=float, default=0.5)\n",
        "#     ap.add_argument(\"--scales\", nargs=\"+\", type=float, default=[0.75,1.0,1.25])\n",
        "#     args = ap.parse_args()\n",
        "\n",
        "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#     processor, model = load_dinov3(args.model_id, dtype=torch.bfloat16, device=device)\n",
        "\n",
        "#     if args.satellite_mode and model.vit.embeddings.patch_embeddings.projection.in_channels == 3:\n",
        "#         model = adapt_vit_patch_embed_to_4ch(model)\n",
        "\n",
        "#     # Build prototype\n",
        "#     chip_embs = []\n",
        "#     for cp in args.chips:\n",
        "#         chip = read_tiff_4ch(cp)  # CHW normalized\n",
        "#         if chip.shape == 4 and model.vit.embeddings.patch_embeddings.projection.in_channels == 3:\n",
        "#             # fallback: drop NIR quickly\n",
        "#             chip = chip[:3, ...]\n",
        "#         emb = embed_chip(model, processor, chip.to(device), device)\n",
        "#         chip_embs.append(emb)\n",
        "#     proto = compute_query_prototype(chip_embs)  # [D]\n",
        "\n",
        "#     results = []\n",
        "#     for fn in sorted(os.listdir(args.targets_dir)):\n",
        "#         if not fn.lower().endswith((\".tif\", \".tiff\", \".jp2\")):\n",
        "#             continue\n",
        "#         path = os.path.join(args.targets_dir, fn)\n",
        "#         img = read_tiff_4ch(path)\n",
        "#         if img.shape == 4 and model.vit.embeddings.patch_embeddings.projection.in_channels == 3:\n",
        "#             img = img[:3, ...]  # fallback\n",
        "#         all_boxes, all_scores = [], []\n",
        "#         for s in args.scales:\n",
        "#             im_s, orig_hw, s_hw = resize_chw(img, s)\n",
        "#             Hs, Ws = s_hw\n",
        "#             inputs = to_model_inputs(processor, im_s.to(device), device)\n",
        "#             with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "#                 outputs = model(**inputs)\n",
        "#             grid = vit_tokens_to_grid(outputs, Hs, Ws, patch=16, num_registers=4)  # [1,h,w,D]\n",
        "#             heat = cosine_heatmap(grid, proto).squeeze(0).detach().cpu().numpy()\n",
        "#             boxes, scores = heatmap_to_boxes(heat, thr=args.thr, scale=16)\n",
        "#             # rescale boxes back to original pixels\n",
        "#             if len(boxes):\n",
        "#                 scale_back_x = orig_hw[1] / s_hw[1]\n",
        "#                 scale_back_y = orig_hw / s_hw\n",
        "#                 boxes[:, [0,2]] *= scale_back_x\n",
        "#                 boxes[:, [1,3]] *= scale_back_y\n",
        "#                 all_boxes.append(boxes)\n",
        "#                 all_scores.append(scores)\n",
        "#         if len(all_boxes):\n",
        "#             boxes_np = np.concatenate(all_boxes, axis=0)\n",
        "#             scores_np = np.concatenate(all_scores, axis=0)\n",
        "#             boxes_np, scores_np = nms_boxes(boxes_np, scores_np, iou_thr=args.nms)\n",
        "#             for b, sc in zip(boxes_np, scores_np):\n",
        "#                 x1,y1,x2,y2 = map(int, b.tolist())\n",
        "#                 results.append([args.object_name, x1, y1, x2, y2, fn, float(sc)])\n",
        "\n",
        "#     with open(args.out, \"w\") as f:\n",
        "#         for rec in results:\n",
        "#             f.write(\"{} {} {} {} {} {} {:.6f}\\n\".format(*rec))\n",
        "#     print(f\"Wrote {len(results)} detections to {args.out}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6V1kQZdpfSXS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ffa56e8b80b402fa5d59882ff5946b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b05ae6fe3ea453499dfefcf7e3e6439",
              "IPY_MODEL_3a4c7c6ef6554b28b9bcc16b16201deb",
              "IPY_MODEL_8dafada2c0fe446d9a2150f63ddde520",
              "IPY_MODEL_a0350b45f8474dd19bd8652f8a80f23b",
              "IPY_MODEL_ea4c76d3af0942b482c85497ccefd7ee"
            ],
            "layout": "IPY_MODEL_d8117b1398224536af15b05be6b3d6df"
          }
        },
        "1b05ae6fe3ea453499dfefcf7e3e6439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0d3306c0584344a00ac2888965ea2b",
            "placeholder": "​",
            "style": "IPY_MODEL_a5da411081e14e95aac91af6285f9e7b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "388d13ed0d2648738afd3c4db0a636ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4c7c6ef6554b28b9bcc16b16201deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7b89dfa8372042dd83fd95cfb7c03847",
            "placeholder": "​",
            "style": "IPY_MODEL_c23c5a0f2e06417cbe63799680a9a3d5",
            "value": ""
          }
        },
        "7b89dfa8372042dd83fd95cfb7c03847": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dafada2c0fe446d9a2150f63ddde520": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a7ed36929cc04d3593f99c64936b0370",
            "style": "IPY_MODEL_9fe3ca4d9f0b48679c28ca02cc66bc7a",
            "value": true
          }
        },
        "9fe3ca4d9f0b48679c28ca02cc66bc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0350b45f8474dd19bd8652f8a80f23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_388d13ed0d2648738afd3c4db0a636ba",
            "style": "IPY_MODEL_e109626c674c418782e0a786cff94222",
            "tooltip": ""
          }
        },
        "a5da411081e14e95aac91af6285f9e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7ed36929cc04d3593f99c64936b0370": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c23c5a0f2e06417cbe63799680a9a3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb0d3306c0584344a00ac2888965ea2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8117b1398224536af15b05be6b3d6df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d82e125834234827b78cced723c9bb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e109626c674c418782e0a786cff94222": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ea4c76d3af0942b482c85497ccefd7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d82e125834234827b78cced723c9bb5d",
            "placeholder": "​",
            "style": "IPY_MODEL_ff0f8a3eeeba48f1b16d166c2a5e5a9b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ff0f8a3eeeba48f1b16d166c2a5e5a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
